{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lemmatization_nltk.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShotaroBaba/NLP_Practice/blob/master/lemmatization_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GlDhQ1ThLul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import nltk lemmatizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "lemmatizer = WordNetLemmatizer() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUWqow6gj7LM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import requests \n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "urlToWiki = \"http://qwone.com//~jason/20Newsgroups/20news-18828.tar.gz\"\n",
        "savedFileName = '20news-18828.tar.gz'\n",
        "savedFileDir = '20news-18828'\n",
        "if not os.path.isfile(savedFileName):\n",
        "  with open(savedFileName,'wb') as output:\n",
        "    output.write(requests.get(urlToWiki).content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFEZ_oO8hM0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "# Extract tar.gz file\n",
        "tarfile.open(savedFileName, \"r:gz\").extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKJA7oVDorls",
        "colab_type": "code",
        "outputId": "edc639db-4975-462c-a111-564706c85200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20news-18828  20news-18828.tar.gz  processed_text.json\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwrixthklEgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_files_path = [os.path.join(root,x) for root, _, files in os.walk(savedFileDir) for x in files]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlZYaHWPlzt0",
        "colab_type": "code",
        "outputId": "41642c4c-9562-4939-a2e6-a9fc4f5bc274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Check if the files are read correctly:\n",
        "print(len(all_files_path))\n",
        "print(all_files_path[1])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18828\n",
            "20news-18828/rec.autos/102836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlEEV3vQqgdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return texts from path\n",
        "# if the text is too short, then it will omit it. \n",
        "def fetch_text(path):\n",
        "  with open(path, encoding=\"utf8\", errors='ignore') as f:\n",
        "    text = f.read()\n",
        "    if len(text.split()) < 50:\n",
        "      return False\n",
        "    else:\n",
        "      return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akeNJS7-l5Id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_texts = list(filter(None, [fetch_text(x) for x in all_files_path]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pITZOQfS1LAc",
        "colab_type": "code",
        "outputId": "f6baa803-7011-4a4e-c2ef-c77db7ff9551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# First, tokenize all the words and remove all stopwords.\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords \n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "custom_stop_words_article = set([\"subject\", \"use\", \"go\", \"maxaxaxaxaxaxaxaxaxaxaxaxaxaxax\", \"don't\", \"isn't\", \"aren't\", \"doesn't\", \n",
        "                                 \"weren't\", \"wasn't\", \"article\", \"writes\", \"would\", \"will\", \"one\", \"n't\", \"from\", \"get\", \"say\", \"know\",\n",
        "                                 \"think\", \"go\", \"the\"])\n",
        "\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "stop_words = [w.translate(table) for w in list(set(stopwords.words('english')).union(custom_stop_words_article))]\n",
        "\n",
        "# Define the tokenization function first.\n",
        "def tokenize_text(text):\n",
        "  text = text.translate(table).lower()\n",
        "  return [w for w in word_tokenize(text) if not w in stop_words and not w in string.punctuation and len(w) > 3]\n",
        "\n",
        "all_processed_text = [tokenize_text(text) for text in all_texts]\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt18MC1k3RUS",
        "colab_type": "code",
        "outputId": "0e695bfe-ead8-4fc8-ac1f-a829caa3eac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "# Display the words of an example processed text.\n",
        "\n",
        "print('\\n'.join(all_processed_text[2][:30]))\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "blfapollohpcom\n",
            "barry\n",
            "frishberg\n",
            "best\n",
            "radar\n",
            "detector\n",
            "valentine1\n",
            "1993apr2119025114371sequentcom\n",
            "troysequentcom\n",
            "troy\n",
            "wecker\n",
            "earlier\n",
            "commented\n",
            "valentine1\n",
            "jimfcenterlinecom\n",
            "replied\n",
            "value\n",
            "units\n",
            "bogey\n",
            "counter\n",
            "really\n",
            "called\n",
            "feature\n",
            "gimmick\n",
            "explanation\n",
            "thanks\n",
            "good\n",
            "comments\n",
            "opinion\n",
            "understand\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYClsRlOCgr",
        "colab_type": "code",
        "outputId": "9d6bfed8-9cf3-47a0-b142-823ca13b4339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Lemmatize with POS Tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict[tag] if tag in [\"J\", \"N\", \"V\", \"R\"] else \"n\"\n",
        "\n",
        "all_processed_text = [[lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in token_list] for token_list in all_processed_text]"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTcwjUu77WBr",
        "colab_type": "code",
        "outputId": "770116ef-9e48-4c1c-ea50-a91de906b6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "print(\"\\n\".join(all_processed_text[2][:30]))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['no', 'himself', 'once', 'weren', 'wouldnt', 'same', 'ma', 'wasn', 'until', 'by', 'theirs', 'had', 'an', 'this', 'few', 'more', 'of', 'nor', 'any', 's', 'up', 'o', 'than', 'its', 'above', 'be', 'my', 'against', 'i', 'below', 'will', 'would', 'can', 'hadn', 'for', 'when', 'very', 'has', 'myself', 'wont', 'hadnt', 'didnt', 'maxaxaxaxaxaxaxaxaxaxaxaxaxaxax', 'wouldn', 'doesn', 'aren', 'are', 'subject', 'while', 'your', 'herself', 'at', 'which', 'their', 'a', 've', 'from', 'our', 'off', 'd', 'in', 'such', 'shant', 'youre', 'y', 'not', 'neednt', 'use', 'who', 'how', 'down', 'isn', 'itself', 'the', 'but', 't', 'during', 'into', 'both', 'we', 'his', 'her', 'now', 'think', 'ours', 'between', 'been', 'go', 'then', 'say', 'm', 'what', 'own', 'youd', 'hers', 'after', 'nt', 're', 'she', 'me', 'am', 'was', 'if', 'get', 'won', 'couldnt', 'ourselves', 'out', 'you', 'over', 'these', 'just', 'shouldve', 'doesnt', 'here', 'only', 'll', 'mightnt', 'were', 'it', 'all', 'yours', 'doing', 'haven', 'one', 'did', 'or', 'because', 'thatll', 'through', 'don', 'should', 'didn', 'having', 'as', 'werent', 'article', 'under', 'isnt', 'do', 'hasn', 'wasnt', 'mightn', 'too', 'arent', 'where', 'shan', 'those', 'to', 'shes', 'writes', 'does', 'yourselves', 'havent', 'mustnt', 'most', 'shouldn', 'him', 'they', 'them', 'shouldnt', 'themselves', 'know', 'have', 'with', 'its', 'dont', 'that', 'hasnt', 'and', 'other', 'youve', 'each', 'being', 'again', 'whom', 'further', 'on', 'there', 'some', 'is', 'before', 'he', 'yourself', 'youll', 'needn', 'why', 'so', 'ain', 'mustn', 'couldn', 'about']\n",
            "blfapollohpcom\n",
            "barry\n",
            "frishberg\n",
            "best\n",
            "radar\n",
            "detector\n",
            "valentine1\n",
            "1993apr2119025114371sequentcom\n",
            "troysequentcom\n",
            "troy\n",
            "wecker\n",
            "earlier\n",
            "comment\n",
            "valentine1\n",
            "jimfcenterlinecom\n",
            "reply\n",
            "value\n",
            "unit\n",
            "bogey\n",
            "counter\n",
            "really\n",
            "call\n",
            "feature\n",
            "gimmick\n",
            "explanation\n",
            "thanks\n",
            "good\n",
            "comment\n",
            "opinion\n",
            "understand\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz_DOAg-QHlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open(\"processed_text.json\", \"w\") as f:\n",
        "  json.dump(all_processed_text, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rwEOgbSV_YF",
        "colab_type": "code",
        "outputId": "6cfd7f5f-ee3b-4ce6-f651-0f214df4863e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20news-18828  20news-18828.tar.gz  processed_text.json\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI4HLHLR7k0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Use LDA for the further analysis."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayPQT_weVubl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "dictionary = Dictionary(all_processed_text)\n",
        "corpus = [dictionary.doc2bow(x) for x in all_processed_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcAZ1IJUYMGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import LdaMulticore\n",
        "\n",
        "newsdata_topics = LdaMulticore(corpus, id2word=dictionary, num_topics=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IH_KFwCYiG3",
        "colab_type": "code",
        "outputId": "021a93a4-d894-4b1d-c034-4a20f1454727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "# Shot the topics of all images\n",
        "\n",
        "newsdata_topics.show_topics(num_words=20)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.004*\"make\" + 0.004*\"need\" + 0.004*\"like\" + 0.004*\"well\" + 0.004*\"time\" + 0.004*\"window\" + 0.003*\"also\" + 0.003*\"chip\" + 0.003*\"could\" + 0.003*\"people\" + 0.003*\"work\" + 0.003*\"question\" + 0.003*\"even\" + 0.003*\"take\" + 0.003*\"say\" + 0.003*\"number\" + 0.002*\"give\" + 0.002*\"use\" + 0.002*\"david\" + 0.002*\"thing\"'),\n",
              " (1,\n",
              "  '0.010*\"jpeg\" + 0.006*\"image\" + 0.006*\"file\" + 0.004*\"program\" + 0.004*\"use\" + 0.003*\"post\" + 0.003*\"make\" + 0.003*\"also\" + 0.003*\"like\" + 0.003*\"people\" + 0.003*\"system\" + 0.003*\"need\" + 0.003*\"data\" + 0.003*\"time\" + 0.003*\"code\" + 0.003*\"work\" + 0.003*\"group\" + 0.003*\"even\" + 0.002*\"well\" + 0.002*\"include\"'),\n",
              " (2,\n",
              "  '0.003*\"well\" + 0.003*\"time\" + 0.003*\"like\" + 0.003*\"say\" + 0.003*\"point\" + 0.003*\"need\" + 0.002*\"want\" + 0.002*\"right\" + 0.002*\"people\" + 0.002*\"work\" + 0.002*\"good\" + 0.002*\"give\" + 0.002*\"university\" + 0.002*\"take\" + 0.002*\"make\" + 0.002*\"use\" + 0.002*\"thing\" + 0.002*\"could\" + 0.002*\"game\" + 0.002*\"also\"'),\n",
              " (3,\n",
              "  '0.006*\"israel\" + 0.004*\"book\" + 0.004*\"armenian\" + 0.003*\"jesus\" + 0.003*\"arab\" + 0.003*\"time\" + 0.002*\"year\" + 0.002*\"like\" + 0.002*\"university\" + 0.002*\"say\" + 0.002*\"well\" + 0.002*\"also\" + 0.002*\"take\" + 0.002*\"first\" + 0.002*\"people\" + 0.002*\"good\" + 0.002*\"use\" + 0.002*\"could\" + 0.002*\"email\" + 0.002*\"make\"'),\n",
              " (4,\n",
              "  '0.005*\"say\" + 0.005*\"armenian\" + 0.005*\"turkish\" + 0.004*\"turkey\" + 0.003*\"palestinian\" + 0.003*\"people\" + 0.003*\"right\" + 0.003*\"also\" + 0.003*\"image\" + 0.003*\"state\" + 0.003*\"look\" + 0.003*\"file\" + 0.003*\"take\" + 0.003*\"university\" + 0.003*\"time\" + 0.003*\"information\" + 0.003*\"well\" + 0.002*\"could\" + 0.002*\"give\" + 0.002*\"greek\"'),\n",
              " (5,\n",
              "  '0.007*\"people\" + 0.006*\"armenian\" + 0.006*\"say\" + 0.006*\"jew\" + 0.005*\"israeli\" + 0.005*\"arab\" + 0.005*\"muslim\" + 0.004*\"make\" + 0.004*\"take\" + 0.004*\"kill\" + 0.004*\"like\" + 0.004*\"go\" + 0.004*\"come\" + 0.003*\"time\" + 0.003*\"well\" + 0.003*\"right\" + 0.003*\"israel\" + 0.003*\"many\" + 0.003*\"give\" + 0.003*\"armenia\"'),\n",
              " (6,\n",
              "  '0.008*\"image\" + 0.007*\"file\" + 0.006*\"graphic\" + 0.005*\"program\" + 0.005*\"use\" + 0.005*\"color\" + 0.005*\"system\" + 0.004*\"also\" + 0.004*\"window\" + 0.004*\"version\" + 0.004*\"like\" + 0.004*\"card\" + 0.004*\"list\" + 0.003*\"software\" + 0.003*\"work\" + 0.003*\"display\" + 0.003*\"anyone\" + 0.003*\"need\" + 0.003*\"problem\" + 0.003*\"write\"'),\n",
              " (7,\n",
              "  '0.006*\"people\" + 0.005*\"like\" + 0.004*\"israel\" + 0.004*\"make\" + 0.004*\"say\" + 0.003*\"well\" + 0.003*\"year\" + 0.003*\"come\" + 0.003*\"also\" + 0.003*\"even\" + 0.003*\"time\" + 0.003*\"thing\" + 0.003*\"go\" + 0.003*\"want\" + 0.003*\"work\" + 0.003*\"christian\" + 0.003*\"give\" + 0.003*\"could\" + 0.003*\"take\" + 0.003*\"armenian\"'),\n",
              " (8,\n",
              "  '0.006*\"turkish\" + 0.005*\"problem\" + 0.005*\"people\" + 0.005*\"like\" + 0.005*\"time\" + 0.005*\"make\" + 0.004*\"well\" + 0.004*\"question\" + 0.004*\"take\" + 0.003*\"go\" + 0.003*\"right\" + 0.003*\"work\" + 0.003*\"use\" + 0.003*\"want\" + 0.003*\"many\" + 0.003*\"help\" + 0.003*\"window\" + 0.003*\"believe\" + 0.003*\"need\" + 0.003*\"bike\"'),\n",
              " (9,\n",
              "  '0.008*\"file\" + 0.005*\"format\" + 0.005*\"image\" + 0.005*\"system\" + 0.005*\"window\" + 0.004*\"software\" + 0.004*\"use\" + 0.004*\"program\" + 0.004*\"work\" + 0.004*\"like\" + 0.003*\"drive\" + 0.003*\"email\" + 0.003*\"graphic\" + 0.003*\"available\" + 0.003*\"also\" + 0.003*\"computer\" + 0.003*\"please\" + 0.003*\"need\" + 0.003*\"well\" + 0.003*\"version\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D__7usFuhbDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference  Lang K., & Rennie J. (1997). The 20 newsgroups data set . Retrieved from http://qwone.com/~jason/20Newsgroups/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}