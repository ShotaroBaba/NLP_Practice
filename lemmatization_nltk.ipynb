{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lemmatization_nltk.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShotaroBaba/NLP_Practice/blob/NLTK_test/lemmatization_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GlDhQ1ThLul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import nltk lemmatizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "lemmatizer = WordNetLemmatizer() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUWqow6gj7LM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import requests \n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "urlToWiki = \"http://qwone.com//~jason/20Newsgroups/20news-18828.tar.gz\"\n",
        "savedFileName = '20news-18828.tar.gz'\n",
        "savedFileDir = '20news-18828'\n",
        "if not os.path.isfile(savedFileName):\n",
        "  with open(savedFileName,'wb') as output:\n",
        "    output.write(requests.get(urlToWiki).content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFEZ_oO8hM0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "# Extract tar.gz file\n",
        "tarfile.open(savedFileName, \"r:gz\").extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKJA7oVDorls",
        "colab_type": "code",
        "outputId": "8ef1654f-39d1-4dd5-f31b-0d9965815b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20news-18828  20news-18828.tar.gz  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwrixthklEgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_files_path = [os.path.join(root,x) for root, _, files in os.walk(savedFileDir) for x in files]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlZYaHWPlzt0",
        "colab_type": "code",
        "outputId": "be8cc4c2-b77a-4a4c-e1d4-bf4cbd74a2da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Check if the files are read correctly:\n",
        "print(len(all_files_path))\n",
        "print(all_files_path[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18828\n",
            "20news-18828/rec.autos/102836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlEEV3vQqgdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return texts from path\n",
        "# if the text is too short, then it will omit it. \n",
        "def fetch_text(path):\n",
        "  with open(path, encoding=\"utf8\", errors='ignore') as f:\n",
        "    text = f.read()\n",
        "    if len(text.split()) < 50:\n",
        "      return False\n",
        "    else:\n",
        "      return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akeNJS7-l5Id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_texts = list(filter(None, [fetch_text(x) for x in all_files_path]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pITZOQfS1LAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "49f295c2-a59d-4fb9-d6bf-c481bf7eaf53"
      },
      "source": [
        "# First, tokenize all the words and remove all stopwords.\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords \n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# Define the tokenization function first.\n",
        "def tokenize_text(text):\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  text = text.translate(table)\n",
        "  return [w.lower() for w in word_tokenize(text) if not w in stop_words and not w in string.punctuation]\n",
        "\n",
        "all_processed_text = [tokenize_text(text) for text in all_texts]\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt18MC1k3RUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ac334c24-8abe-4da5-fac4-01d9d0c2f4fa"
      },
      "source": [
        "# Display the words of an example processed text.\n",
        "\n",
        "print(all_processed_text[2])\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['from', 'blfapollohpcom', 'barry', 'frishberg', 'subject', 're', 'best', 'radar', 'detector', 'valentine1', 'from', 'article', '1993apr2119025114371sequentcom', 'troysequentcom', 'troy', 'wecker', 'earlier', 'i', 'commented', 'valentine1', 'jimfcenterlinecom', 'replied', 'value', 'units', 'bogey', 'counter', 'i', 'didnt', 'really', 'go', 'i', 'called', 'feature', 'gimmick', 'explanation', 'thanks', 'jim', 'good', 'comments', 'opinion', 'from', 'i', 'understand', 'valentine1', 'tell', 'microwaves', 'coming', 'front', 'rear', 'there', 'two', 'antennas', 'if', 'coming', 'interpreted', 'side', 'bogey', 'bogey', 'counts', 'determined', 'sources', 'relative', 'strength', 'one', 'another', 'even', 'reflections', 'source', 'it', 'sounds', 'like', 'analysis', 'based', 'hypothesis', 'actually', 'using', 'valentine1', 'id', 'like', 'give', 'feedback', 'based', 'real', 'life', 'experince', 'i', 'keep', 'valentine1', 'advanced', 'logic', 'mode', 'rarely', 'lights', 'christmas', 'tree', 'the', 'time', 'i', 'middle', 'major', 'shopping', 'area', 'makes', 'sense', 'since', '8', 'sources', 'coming', 'many', 'different', 'directions', 'i', 'found', 'valentine1', 'consistent', 'reporting', 'bogeys', 'regardless', 'moving', 'cars', 'area', 'i', 'found', 'directional', 'indication', 'useful', 'in', 'one', 'case', 'two', 'radar', 'traps', 'set', 'within', 'one', 'mile', 'as', 'i', 'passed', 'first', 'radar', 'trap', 'direction', 'indication', 'changed', 'then', 'detector', 'set', 'pointing', 'forward', 'direction', 'with', 'radar', 'detectors', 'i', 'would', 'assumed', 'due', 'reflection', 'but', 'valentine1', 'i', 'knew', 'high', 'probability', 'another', 'trap', 'and', 'on', 'occasions', 'directional', 'helped', 'discern', 'false', 'alarm', 'true', 'alarm', 'for', 'example', 'i', 'pass', 'source', 'direction', 'indicator', 'changes', 'the', 'directional', 'also', 'allows', 'focus', 'attention', 'signal', 'might', 'coming', 'instead', 'look', 'place', 'when', 'car', 'approaching', 'rear', 'detector', 'leaks', 'i', 'tell', 'signal', 'coming', 'rear', 'car', 'passes', 'i', 'verify', 'source', 'with', 'detectors', 'i', 'would', 'unable', 'would', 'assume', 'radar', 'trap', 'none', 'ive', 'valentine1', 'several', 'months', 'find', 'added', 'features', 'useful', 'gimmicks', 'barry', 'barry', 'frishberg', 'blfapollochhpcom', 'chelmsforduk', 'expert', 'center', '508tn', '4364319', 'chelmsford', 'ma']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYClsRlOCgr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c2b5e1ee-a880-4612-fa01-a819cbe64b82"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Lemmatize with POS Tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict[tag] if tag in [\"J\", \"N\", \"V\", \"R\"] else \"n\"\n",
        "\n",
        "all_processed_texts = [[lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in token_list] for token_list in all_processed_text]"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTcwjUu77WBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "b4028a41-4341-4e8b-91f8-90b31c16780a"
      },
      "source": [
        "print(\"\\n\".join(all_processed_texts[2][:30]))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from\n",
            "blfapollohpcom\n",
            "barry\n",
            "frishberg\n",
            "subject\n",
            "re\n",
            "best\n",
            "radar\n",
            "detector\n",
            "valentine1\n",
            "from\n",
            "article\n",
            "1993apr2119025114371sequentcom\n",
            "troysequentcom\n",
            "troy\n",
            "wecker\n",
            "earlier\n",
            "i\n",
            "comment\n",
            "valentine1\n",
            "jimfcenterlinecom\n",
            "reply\n",
            "value\n",
            "unit\n",
            "bogey\n",
            "counter\n",
            "i\n",
            "didnt\n",
            "really\n",
            "go\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz_DOAg-QHlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open(\"processed_text.json\", \"w\") as f:\n",
        "  json.dump(all_processed_texts, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rwEOgbSV_YF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70d60ea6-ac09-4b23-c850-040e51bf2db0"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20news-18828  20news-18828.tar.gz  processed_text.json\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI4HLHLR7k0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# TODO: Use LDA for the further analysis."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D__7usFuhbDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference  Lang K., & Rennie J. (1997). The 20 newsgroups data set . Retrieved from http://qwone.com/~jason/20Newsgroups/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}