{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lemmatization_nltk.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShotaroBaba/NLP_Practice/blob/NLTK_test/lemmatization_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GlDhQ1ThLul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import nltk lemmatizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "lemmatizer = WordNetLemmatizer() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUWqow6gj7LM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import requests \n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "urlToWiki = \"http://qwone.com//~jason/20Newsgroups/20news-18828.tar.gz\"\n",
        "savedFileName = '20news-18828.tar.gz'\n",
        "savedFileDir = '20news-18828'\n",
        "if not os.path.isfile(savedFileName):\n",
        "  with open(savedFileName,'wb') as output:\n",
        "    output.write(requests.get(urlToWiki).content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFEZ_oO8hM0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "# Extract tar.gz file\n",
        "tarfile.open(savedFileName, \"r:gz\").extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKJA7oVDorls",
        "colab_type": "code",
        "outputId": "0931a5f4-db97-4362-d6a9-2eccf52c61cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20news-18828  20news-18828.tar.gz  processed_text.json\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwrixthklEgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_files_path = [os.path.join(root,x) for root, _, files in os.walk(savedFileDir) for x in files]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlZYaHWPlzt0",
        "colab_type": "code",
        "outputId": "d64d88c6-0051-47f6-f5df-13202ab40c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Check if the files are read correctly:\n",
        "print(len(all_files_path))\n",
        "print(all_files_path[1])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18828\n",
            "20news-18828/rec.autos/102836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlEEV3vQqgdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return texts from path\n",
        "# if the text is too short, then it will omit it. \n",
        "def fetch_text(path):\n",
        "  with open(path, encoding=\"utf8\", errors='ignore') as f:\n",
        "    text = f.read()\n",
        "    if len(text.split()) < 50:\n",
        "      return False\n",
        "    else:\n",
        "      return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akeNJS7-l5Id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_texts = list(filter(None, [fetch_text(x) for x in all_files_path]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pITZOQfS1LAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "bc33ebd7-f7c1-4e37-d78d-355df8d294ac"
      },
      "source": [
        "# First, tokenize all the words and remove all stopwords.\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords \n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# Define the tokenization function first.\n",
        "def tokenize_text(text):\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  text = text.translate(table)\n",
        "  return [w.lower() for w in word_tokenize(text) if not w in stop_words and not w in string.punctuation]\n",
        "\n",
        "all_processed_text = [tokenize_text(text) for text in all_texts]\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt18MC1k3RUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "77cf8e36-7c71-46a5-f81c-2c2b3cbfe3d2"
      },
      "source": [
        "# Display the words of an example processed text.\n",
        "\n",
        "print('\\n'.join(all_processed_text[2][:30]))\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from\n",
            "blfapollohpcom\n",
            "barry\n",
            "frishberg\n",
            "subject\n",
            "re\n",
            "best\n",
            "radar\n",
            "detector\n",
            "valentine1\n",
            "from\n",
            "article\n",
            "1993apr2119025114371sequentcom\n",
            "troysequentcom\n",
            "troy\n",
            "wecker\n",
            "earlier\n",
            "i\n",
            "commented\n",
            "valentine1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYClsRlOCgr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7c97e6f0-c8b3-4154-aeac-fb6489298338"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Lemmatize with POS Tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict[tag] if tag in [\"J\", \"N\", \"V\", \"R\"] else \"n\"\n",
        "\n",
        "all_processed_texts = [[lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in token_list] for token_list in all_processed_text]"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTcwjUu77WBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "8f2d86ee-44ab-42dc-c3b3-8eaa2989c7ba"
      },
      "source": [
        "print(\"\\n\".join(all_processed_texts[2][:30]))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from\n",
            "blfapollohpcom\n",
            "barry\n",
            "frishberg\n",
            "subject\n",
            "re\n",
            "best\n",
            "radar\n",
            "detector\n",
            "valentine1\n",
            "from\n",
            "article\n",
            "1993apr2119025114371sequentcom\n",
            "troysequentcom\n",
            "troy\n",
            "wecker\n",
            "earlier\n",
            "i\n",
            "comment\n",
            "valentine1\n",
            "jimfcenterlinecom\n",
            "reply\n",
            "value\n",
            "unit\n",
            "bogey\n",
            "counter\n",
            "i\n",
            "didnt\n",
            "really\n",
            "go\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz_DOAg-QHlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open(\"processed_text.json\", \"w\") as f:\n",
        "  json.dump(all_processed_texts, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rwEOgbSV_YF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1d9e51a-98ab-47c9-add5-2bb77fe82ca6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20news-18828  20news-18828.tar.gz  processed_text.json\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI4HLHLR7k0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Use LDA for the further analysis."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D__7usFuhbDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference  Lang K., & Rennie J. (1997). The 20 newsgroups data set . Retrieved from http://qwone.com/~jason/20Newsgroups/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}